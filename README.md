<h4 align="center"> 
	ğŸš§ Projeto para anÃ¡lise de dados 1.0 ğŸš€ em construÃ§Ã£o... ğŸš§
</h4>

# Projeto AnÃ¡lise de dados ğŸ”¥

## ğŸ’» Sobre o projeto

Nesse projeto serÃ¡ demonstrado como criar uma pipelines para ingestÃ£o, processamento, anÃ¡lise e consumo de dados utilizando algumas das soluÃ§Ãµes mais utilizadas no mercado atualmente. Para isso utilizaremos:
- Python Flask (AplicaÃ§Ã£o principal que serÃ¡ acessada pelos usuÃ¡rios finais)
- PostegreSQL (Banco de dados que receberÃ¡ os dados da aplicaÃ§Ã£o)
- Debezium (AplicaÃ§Ã£o responsÃ¡vel por coletar os dados do banco de dados e enviar para um tÃ³pico do Kafka)
- Apache Kafka (Camada de fila de dados/eventos)
- Apache Flink (Enriquecimento e validaÃ§Ã£o de dados)
- Apache Druid (AgregaÃ§Ã£o e Armazenamento)
- Elasticsearch (Motor de busca/Armazenamento)
- AplicaÃ§Ã£o Python Consumer (AplicaÃ§Ã£o responsÃ¡vel por ler os eventos do Kafka e enviar para o Elasticsearch)
- Apache Superset (Painel analÃ­tico para monitoramento do fluxo de dados)
- Docker (GestÃ£o dos containers do projeto)

### Diagrama v1
![projeto-dados](apoio/projeto_dados.png)

### Web

Exemplo da pÃ¡gina Web que farÃ¡ a comunicaÃ§Ã£o com o PostgreSQL

![projeto-dados](apoio/webpage.png)

ReferÃªncia do cÃ³digo: [W3Docs](https://www.w3docs.com/tools/editor/5795)

## ğŸ›  Tecnologias

As seguintes ferramentas foram usadas na construÃ§Ã£o do projeto:

- [Python Flask][flask]
- [PostgreSQL][postgresql]
- [Debezium][debezium]
- [Apache Kafka][kafka]
- [Apache Flink][flink]
- [Apache Druid][druid]
- [Elasticsearch][elasticsearch]
- [AplicaÃ§Ã£o Python Consumer][python-kafka]
- [Apache Superset][superset]
- [Docker][Docker]

### PrÃ©-requisitos

Antes de comeÃ§ar, vocÃª vai precisar ter instalado em sua mÃ¡quina as seguintes ferramentas:

- [Docker][Docker]
- Docker-compose versÃ£o 1.29.2 - [How to](https://docs.docker.com/compose/install/)
- AlÃ©m disto Ã© bom ter um editor para trabalhar com o cÃ³digo como [VSCode][vscode], apesar de nÃ£o ser obrigatÃ³rio.

## ğŸš€ Como executar o projeto

1. Clone o repositorio projeto-dados e crie a rede de comunicaÃ§Ã£o interna dos containers

    ```bash
    $ git clone https://github.com/j040n3t0/projeto-dados.git
    $ docker network create custom_network
    ```

2. Execute o compose da pasta projeto-dados/flask_postgresql

    ```bash
    $ cd projeto-dados/flask_postgresql
    $ docker-compose up
    ```

    Feito isso vocÃª jÃ¡ conseguirÃ¡ acessar a pÃ¡gina do frontend no seguinte endereÃ§o: **http://SEU_IP:5000**

    **Troubleshoot**

    ReferÃªncias:
    - wal_level [Must be Logical](https://stackoverflow.com/questions/59416301/how-to-change-postgres-docker-image-wal-level-on-setup)
    - Captcha [codepen](https://codepen.io/manishjanky/pen/eRNKLL)
    

3. Execute o compose da pasta debezium_kafka

    ```bash
    $ cd projeto-dados/debezium_kafka
    $ docker-compose up
    $ curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d @./postgresql-connect.json
    ```

    Nesse ponto foi criado o conector (Debezium) que enviarÃ¡ os dados do PostgreSQL para o tÃ³pico **fullfillment.public.inventory** no Kafka

    **Troubleshoot**

    ReferÃªncia: https://stackoverflow.com/questions/62150625/couldnt-read-data-from-json-config-file-using-curl

4. Apache Flink

    ğŸš§ Pendente! ğŸš§

5. Elasticsearch / python engine

    ```bash
    $ cd projeto-dados/elastic_pythonConsumer
    $ mkdir elastic-data
    $ docker-compose up
    ```

    Nessa etapa do projeto o container do Python jÃ¡ deverÃ¡ estar lendo do tÃ³pico **fullfillment.public.inventory** no Kafka e enviando os dados para o Elasticsearch! O elasticsearch estarÃ¡ acessÃ­vel no seguinte endereÃ§o: **http://SEU_IP:9200**

6. Druid

    ```bash
    $ cd projeto-dados/apache_druid
    $ docker-compose up
    ```

    ![druid](apoio/druid.png)

7. Superset

    ```bash
    $ cd projeto-dados/apache_superset
    $ docker-compose -f docker-compose-non-dev.yml up
    # Esse plugin Ã© necessÃ¡rio para permitir que o superset conecte em bases Elasticsearch
    $ docker exec <id/nomecontainer> bash -c "pip install elasticsearch-dbapi"
    ```

    Nessa etapa do projeto vocÃª tem a integraÃ§Ã£o completa de todos os componentes abordados anteriormente! O Superset estarÃ¡ acessÃ­vel no seguinte endereÃ§o: **http://SEU_IP:8088** com usuÃ¡rio **admin** e senha **admin**. 
    
    Agora Ã© sÃ³ se divertir analisando seu ambiente e montando Dashboards comparativas com o Superset, como no exemplo abaixo podemos ver se as bases de dados do (Cache) Elasticsearch e (BD Prod) PostgreSQL estÃ£o com os mesmo registros.

    ![superset](apoio/superset.png)


## ğŸ’¡ Ideias de melhorias

- [X] Colocar captcha para evitar engraÃ§adinhos
- [X] Limitar a listagem dos registros aos Ãºltimos 10
- [ ] Colocar todos os serviÃ§os na mesma rede, isso evitarÃ¡ publicar portas desnecessÃ¡rias
- [ ] ServiÃ§os que nÃ£o possuem senha, adicionar camada de autenticaÃ§Ã£o (Nginx)
- [ ] Padronizar nome dos containers
- [ ] Remover/alterar senhas padrÃµes

## ğŸ§  Idealizadores

ResponsÃ¡veis pelo projeto:
- JoÃ£o Neto [joaojose.ti@gmail.com]

## ğŸ˜¯ Como contribuir para o projeto

ğŸ‘‹ğŸ½ [Entre em contato!](https://t.me/j040n3t0)

<!-- ## ğŸ“ LicenÃ§a -->


<h4 align="center"> 
	ğŸš§ Projeto para anÃ¡lise de dados 1.0 ğŸš€ em construÃ§Ã£o... ğŸš§
</h4>

[vscode]: https://code.visualstudio.com/
[docker]: https://www.docker.com/
[flask]: https://flask.palletsprojects.com/en/2.0.x/
[postgresql]: https://www.postgresql.org/
[debezium]: https://debezium.io/
[kafka]: https://kafka.apache.org/
[flink]: https://flink.apache.org/
[druid]: https://druid.apache.org/
[superset]: https://superset.apache.org/
[elasticsearch]: https://www.elastic.co/pt/what-is/elasticsearch
[python-kafka]: https://github.com/dpkp/kafka-python